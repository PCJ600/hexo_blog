---
layout: next
title: 项目问题梳理
date: 2020-01-02 17:56:30
categories: interview
tags: interview
---

<!-- toc -->
<!-- more -->

# 项目1: Trend Micro - Service Gateway

## 简单介绍下你的项目
* 我参与的项目是Service Gateway, 一个安装在企业网络的服务网关, 允许本地的趋势产品访问后端服务
* 服务网关由两部分组成: 一个部署在客户网络的Linux虚拟机设备和一个云端的APP
* 虚拟设备安装了Microk8s, 给本地的安全产品提供服务, 减少客户的带宽消耗. 云端的APP包括一个前后端, 用于管理连接的虚拟设备
* 我负责虚拟设备和微服务的设计开发工作, 主要贡献是设计了一个虚拟设备固件的双系统分区升级方案, 解决了开发过程中的痛点, 改善了客户体验. 
* 目前有6k+企业客户, 1w+台设备使用我的升级方案

## 为什么选择双系统全量升级方案?
首先交代背景, 为什么要重新设计原来的方案

我们提到, 虚拟设备安装在客户本地网络中, 虚拟设备的固件和微服务需要定期升级. 固件就是指虚拟机ISO镜像, 包括CentOS+Microk8s+软件
早期采用增量升级方案, 有三个问题很难解决
* 每次升级需要维护两个版本间差异, 导致历史代码积累, 增加开发维护的复杂度
* 无法实现OS迁移，早期我们使用的发行版是CentOS, 面临CentOS停止维护的问题, 一个系统分区很难实现OS的迁移
* 不支持回滚, 升级出现故障后很难恢复, 只能让客户重装，客户体验非常差

为了解决这些问题, 我们自然想到了双系统分区方案. 比如嵌入式设备, 安卓设备都采用A/B分级升级的方案

## 固件的具体内容包括哪些？如何保证其最小化但又能满足功能需求
ISO镜像包括一个RockyLinux官方ISO, Microk8s, 升级模块RPM包, 一个CLI命令行
我们采用了很多策略, 实现镜像的最小化
* 使用RockyLinux 9.4官方的最小发行版定制ISO镜像, 官方镜像只有1.7G
* 选择了Microk8s, 一个轻量化的K8s发行版。 Microk8s插件支持按需加载, 只安装必要的插件
* 客户用OVA安装虚拟设备而不是直接用ISO。 为了最小化OVA
  * 只分配有限空间给构建镜像的虚拟机, 等客户安装成功后再自动分配剩余磁盘空间。 
  * 导出OVA前，清理临时文件，日志文件, 临时关闭swap分区
最终导出的OVA在2.5G左右

## 在线升级整体流程是什么, Python升级模块怎么设计的
在线升级的整体流程时:
* 首先划分双系统分区, 构建虚拟设备镜像, 基于镜像出一个全量的升级包（压缩包)
* 虚拟设备下载升级包，把升级包解压到备用系统分区
* 在通过设置启动项, 切换到备用分区完成升级

为了实现在线升级的功能, 我开发了一个升级模块. 升级模块包含一个Python的后台进程和一个Cronjob脚本, 部署在虚拟设备上
Python daemon的作用是, 通过AWS IoT消息队列, 消费后端发来的升级任务. 设计方法:
* 基于AWS IoT的SDK实现了一个MQ的消费端
* 利用Python中线程安全的队列, 消费者收到消息后，根据消息类型, 把消息分发到不同的队列. 每个消费线程从各自队列中取消息处理.

## 你提到多线程来处理不同类型的任务，但如何保证线程间的同步和数据一致性？
使用Python的Queue处理不同类型任务, 利用Queue的线程安全特性, 保证多个线程同时访问队列不会出现数据竞争问题

## 为什么引入消息队列
* 实现后端和虚拟设备的异步通信。 由于虚拟设备的IP可能变更, 使用HTTP通过IP请求的方式可能会失败
* 减轻后端压力，因为升级任务耗时较长, HTTP交互不适合长时间任务

## 你如何防止重复消费
* 首先每个消息都有一个UUID类型的taskID
  在Python daemon中, 利用Python过期字典，每次消费时, 先获取锁, 检查taskID是否在过期字典中; 如果ID存在说明重复消息; 否则消费消息, 把taskID添加到过期字典
* 第二个是消费消息做到幂等性, 比如升级场景, 如果判断当前版本和目标版本一致, 就不做处理. 保证同一个task执行多次，结果也是一致的。  

## 如何保证消息传递可靠性, 是否有重试机制或消息确认机制 ?
* Aws IoT协议的Qos(服务质量)机制, 支持QOS 1, 至少一次消息传递, 重复发送直到收到PUBACK
* 发送端发送失败后重试3次

## 为什么选择AWS IoT, 有没有考虑其他MQ解决方案 ?
* AWS IoT支持基于Topic的主题分发, 每个虚拟设备通过订阅topic执行升级任务, 功能上可以满足要求
* AWS IoT支持MQTT协议，轻量高效，资源消耗低, 适合本地设备的通信
* AWS IoT是Amazon提供的服务, 经过了企业生产环境验证, 具有稳定性和可靠性, 且我们公司在普遍使用AWS的服务, 选择AWS IoT可以简化开发流程
* 不选择Kafka，是因为我们的并发需求不高, 6k客户,1w+虚拟设备, 主要是低频但关键的任务。 使用Kafka没有明显优势，反而增加复杂性
* 不选择RabbitMQ, 因为这需要自行搭建，维护集群. 增加开发维护成本

## 为什么设计Cronjob，不直接在Python服务里处理升级
这样设计是考虑到职责分离; Python Daemon专注于消息消费和记录任务, Cronjob负责具体任务执行, 这样便于维护和扩展。

## 启动项切换到备用分区的具体实现方式是什么？
使用grubby管理启动项, grubby是一个专门用于管理GRUB的工具，比手动编辑GRUB配置文件更安全高效
实现步骤概括: 划分独立的boot分区, 禁用os_prober, 使用grubby添加启动项, 设置默认启动项, 重装GRUB
理解GRUB工作原理, Linux启动流程, 完成开发和调试工作。 

## 说下Linux启动流程
* 系统加电, BIOS开机自检
* 按照BIOS设定启动的顺序, 查找可启动设备, 通常是硬盘, 把控制权交给GRUB
* GRUB把内核加载到内存，挂载initrd, 通过initrd加载真正的根文件系统
* 内核启动完成后, 执行第一个用户空间进程init, init负责启动其他服务

## 说下GRUB工作原理
GRUB是Linux的引导加载程序，负责将内核加载到内存中启动，两阶段运行
一阶段位于磁盘的主引导记录(MBR)中，加载二阶段的core.img; core.img读取grub.cfg, 生成启动菜单, 加载指定的内核和initrd

## 全量升级包的下载和解压过程中，如何处理网络中断或磁盘空间不足的问题 ?
对于网络中断的处理
* 升级时, 虚拟设备从云端获取升级包下载链接和sha256sum校验值。 使用wget -c下载, 支持断点续传。
* 下载完成后, 通过sha256sum校验文件完整性。 如果校验失败，说明下载失败, 此时通知后端下载失败, 提示客户重试。
磁盘空间不足的处理
* 后端每15分钟收集所有虚拟设备的metrics，包括磁盘使用率，内存，CPU负载。 磁盘占用超过80%，通过UI提示客户kongjian 不足。
* 另外, 我们实现了磁盘扩容的功能，支持客户在UI上扩容或者通过CLI里离线扩容

## 为什么选Microk8s, 不选k3s, minukube ?
* 除了Microk8s, 还有minukube, k3s. Minikube只适合本地开发和学习，不是为企业生产环境设计的，缺乏高可用性，排除
* k3s有很多优点, 比如使用二进制文件, 依赖少，资源占用低，社区比Microk8s活跃
* Microk8s的缺点是社区活跃不足, 且安装依赖snapd, 但是这种影响可控。 且项目早期已选择Microk8s,为了保持一致性，减少迁移成本，最终仍然选择了Microk8s

## OS迁移怎么做的
首先说下OS迁移面临的难点
* 从单分区到双分区, 需要重建分区表, 这会破坏当前系统数据. 怎么一键式重建分区是个难点
* 还要考虑扩展性, 后续如果有迁移OS的需求，做到零修改

我们的做法是, 基于CentOS ISO中的initrd做定制, 把升级包复制到临时内存, 对磁盘重建分区, 再把升级包解压到真正根文件系统, 恢复系统配置和登录口令

## 回滚机制是如何实现的？ 还是只能完全回滚整个系统？
客户只需在CLI上敲rollback即可, 我们会通过grubby设置启动项, 重启后启动切换到前一个分区

## 优化了微服务的集成方案, 这个是怎么回事?
早期版本中, 微服务镜像和部署文件以RPM包集成在ISO中。 基础镜像有5G, 客户部署时间很长

优化的方法是, 把微服务从镜像中解耦开, 通过网络下载的方式灵活安装, 具体做法是:
* 给每个微服务分配一个独一无二的serviceCode, 服务网关根据serviceCode创建同名的namespace, 实现资源隔离
* 定义统一的部署标准, 每个微服务需要提供一个tar.gz的部署包, 包括容器镜像和部署yaml, 包括deployment, service, configmap, volume, Ingress, 发布到AWS S3
* 启动Microk8s的Ingress插件，提供统一的服务入口

## 如何配置Ingress的? Nginx如何提供统一入口点的，具体实现方式是什么? 具体实现原理是什么?
* 选择了Microk8s默认的Nginx Ingress插件
* 定义一个DaemonSet部署Nginx Ingress Controller, 在DaemonSet中指定hostNetwork, 让Nginx监听宿主机的80和443端口，从而提供外部统一访问入口
* 这种方式避免了NodePort直接暴露端口，从而简化了微服务管理。
* 除了支持HTTP请求，可以通过在DaemonSet定义TCP端口, 支持TCP负载均衡
* 微服务需要提供ingress.yaml, 指定路径路由, 比如把某个路径的请求转发到某个服务。

## 引入Ingress后，如何确保微服务的安全性？
每个虚拟设备注册时，后端会发送公司的证书和私钥, 虚拟设备拿到证书和私钥, 配置在ingress-cert这个secret中

## CLI怎么开发的
虚拟设备安装后，客户需要登录到设备, 做网络配置，进行注册, 才能连到云端。
我们基于Cisco的开源框架实现了一个命令行工具，客户通过CLI完成网络配置，实现注册功能

## 怎么实现注册的
使用JWT注册方式
* 客户首先在UI上拿到register_token （注: 这个register_token并不是后端生成的，而是服务平台维护的)
* 用户在命令行中输入register_token注册，触发注册请求
* 虚拟设备通过POST请求将CPU,内存,IP信息连同register_token一起发送给后端
* 后端收到请求, 解析请求头的token, 验证签名, 获取customerId
* 校验通过后, 后端为虚拟设备生成一个uuid, 用于唯一标识这台设备
* 后端会再生成一个applianceToken (为了简化处理, 这个token和register_token是一样的)
* 后端把虚拟设备信息存到MySQL数据库, 把虚拟设备ID, Token, 还有消息队列的FQDN返回给虚拟设备
* 虚拟设备成功收到响应后，保存applianceId, applianceToken, 用于后续通信   

## JWT校验流程是什么
JWT验证依赖于服务平台的密钥对，在后端配置服务平台的公钥, 使用公钥解密JWT, 并验证签名，

## 为啥applianceToken和registerToken是一样的呢 ?
为了简化实现, 减少额外的Token生成; 会有这样一个问题: 即使不调用注册API，也可以直接使用这个token访问其他的API
项目中的处理方式是： 只用调用注册接口，数据库才能生成对应的注册记录, 其他请求时会判断数据库是否注册状态

## 为什么选择JWT, 而不是u/p或者session
JWT一种无状态的认证机制, 后端无需查询数据库就能完成身份验证, 且支持跨域认证
Session需要维护一个会话ID到数据库，就没采用这个方案

## JWT过期时间如何设置的，怎么同步的
每天定时从服务平台同步Token, Token设置了两个月过期时间, 过期前10天，立刻请求服务平台刷新Token

## 开发了一组诊断命令，用于快速定位 OS、Kubernetes 和网络相关问题
OS: 查cpu, 内存, 进程, disk
网络: IP, 路由, 网卡, iptables, Route
K8S: IMAGE, namespace, pod, configmap, ingress, deployment, services, volume, Microk8s日志
系统日志: 网络没有问题, 可以支持上传日志, 和Remote Shell

## 能举一个troubleshooting客户案例吗, 你怎么解决的
* 客户的NTP服务器问题，导致时间不正确, 和AWS IoT连接失败
* 客户人为操作失误, 把网卡disconnect了, 导致k8s启动失败
* k8s apiserver证书过期, 导致微服务有问题
* 客户虚拟机配置问题，虚拟CPU不支持AVX指令集, 但物理CPU支持

## 项目有合作吗，怎么合作的
[TODO]

## 项目存在哪些优化空间
比如微服务镜像瘦身, 某些微服务基础镜像有几百M, 把镜像做小可以节省客户安装时间, 提升用户体验  

# 项目2: Forward Proxy Service

## 介绍下Forward Proxy这个项目
这是一个安装在Service Gateway上的微服务, 为本地的趋势终端产品接入云服务提供正向代理功能, 实现本地产品访问互联网的集中管理  

早期, 各个本地产品从云端下载数据, 需要各自访问不同的后端, 只能各自实现一套认证，访问控制方案.
通过引入Forward Proxy服务，所有本地产品连接到这个正向代理即可，实现了集中的认证, 访问控制, 降低了各产品开发和维护成本

## Forward Proxy Service是如何部署的？
通过Microk8s部署, 基于hostNetwork模式, 直接使用宿主机网络栈，监听8080端口，提升性能
核心组件有两部分, 一个是Squid软件, 另一个是Python进程, 感知网络设置和白名单变化, 更新Squid配置文件，重新加载Squid

## 并发量有多少，怎么测试的
8vCPU/12G/500G/1000Mbps
1000台agent并发升级, 流量157M*1000, 30分钟下载完毕
CPU 8%, Mem 58%, Disk 1%，峰值内存7G，瓶颈在带宽
157*8/(30*60) = 700Mbps, 相当于1Gpbs网卡

## 为什么选Squid, 是否有其他替代方案(Nginx, HAProxy)?
* Squid作为老牌正向代理, 在生产环境经过验证，稳定可靠.
结合我们的项目需求，正向代理需要支持认证功能, 基于IP,URL的访问控制, 配置用户代理, 以及多代理转发场景(不同的FQDN走不同的代理)
Squid功能丰富，配置简单, 可以容易实现这些功能
Nginx也能支持这些功能, 但是需要通过三方模块, 配置维护复杂一些. 

## Squid正向代理的原理
默认监听3128端口接收客户端连接
Squid使用多进程模型, 主进程不直接处理客户请求, 而是请求分发给worker进程，每个worker进程处理客户端请求 

## Squid的安全认证是如何实现的？ 为什么选择Basic认证, 还有哪些认证方式?
我们使用Basic认证, 请求头中Authorization: 提供用户名和密码
用户名是本地产品名称+guid, 方便密码, 密码设计(用户名+apikey)做SHA1, apikey只有客户知道, 30天过期
代理是HTTP的, 由于虚拟设备在内部网络环境, 就采用了Basic认证方式, 简单快速

## 为什么Squid代理使用HTTP, 不是HTTPS
如果使用HTTPS, 就需要为每个代理服务器生成SSL证书
虚拟设备安装在内网环境， 内网环境使用HTTP代理即可, 证书管理存在维护复杂性
HTTPS涉及加密, 对性能有影响

## 访问控制的具体实现方式是什么？例如是否基于IP地址、或URL 白名单/黑名单？
通过Squid配置文件中定义ACL, 实现访问控制。 
基于FQDN的访问控制, 预设一个白名单, 只允许白名单中的FQDN通过
允许客户在UI上添加白名单, 白名单信息由后端下发到虚拟设备，保存到ConfigMap
Pod中把ConfigMap挂到文件, 如白名单发生变化，从ConfigMap读取新的白名单，再reconfigure

## 配置用户代理作用是什么，怎么做的
客户出于他的网络管理要求，希望所有虚拟设备流量通过他的代理服务器; Squid支持父级代理, 通过配置cache_peer定义父级代理的地址

## Squid 的缓存机制是否被启用？如果有，具体的缓存策略是什么？
没有手动启动磁盘缓存. Squid默认会启动内存缓存, 根据响应头,请求方法决定是否缓存某个请求
Cache-Control 强缓存, Etag 弱缓存; GET, HEAD可以缓存 
 
## 说一下Stunnel的加密通信方案
Forward Proxy安装在客户本地网络中，客户网络是一个高度不确定的环境。 部分客户配置防火墙规则时会出现一些问题, 比如:
* 不支持通配符FQDN
* 配置错误, 或遗漏某个FQDN
* 客户重新配置防火墙规则需要一定时间, 这段时间服务无法工作, 影响用户体验

有两个问题:
* 1. 减少客户的防火墙配置错误导致的网络不通问题 
* 2. 某些特定的FQDN由于性能原因不适合加密, 需要直出或者走客户代理, 其余FQDN数据需要加密走别的代理出。 需要支持这种多代理转发场景

我们参考了科学上网的方案, 基于Stunnel对Squid数据做TLS加密, 从而将终端到云端访问整合为一个FQDN, 简化客户防火墙配置

## Stunnel工作原理
* 基于TLS加密隧道工具, 客户将未加密流量发到Squid, Squid发送到Stunnel
* Stunnel client端使用TLS加密, 传到Stunnel Server, 防火墙只能看到TCP/443的包，只需要允许Stunnel Server的1个FQDN通过即可
* Stunnel Server解密，把HTTP请求送给目标服务

## 举一个定位案例
写一个KB，让客户先排查是否防火墙配置问题
* 首先检查网络设置和网络连接: ip, netmask, gateway, dns, ping gateway
* 让客户检查防火墙规则: 协议，方向, 行为, 规则顺序
* 禁用防火墙的HTTPS证书替换, 某些防火墙为了检测HTTPS流量, 会做中间人拦截HTTPS流量。 防火墙生成一个新的SSL证书，冒充目标网站证书

举例: 某些客户防火墙为了检测HTTPS流量, 会做中间人拦截HTTPS流量。 防火墙生成一个新证书，冒充目标网站证书, 因为虚拟设备不支持客户自定义CA，导致网络不通
解决方法: CLI中实现诊断网络的命令，通过查看curl的结果, 看issuer是否为entrust, Amazon; 如果issuer是一个IP,hostname或防火墙产商名字，说明被替换了

**DeployMent**
AWS云, 每个site一个EKS
site: US, EU, SG, AU, IN, JP, UAE
site: US-EAST-1
vpc cidr: 10.131.44.0/22
subnets: 
xdr-app-tgw-private-1a	10.131.45.0/24
xdr-app-tgw-private-1b	10.131.46.0/24
xdr-app-tgw-private-1c	10.131.47.0/24
xdr-app-tgw-public-1a	10.131.44.0/25
xdr-app-tgw-public-1b	10.131.44.128/25
routes: 
rtb-03eb993eb0a0df54b
0.0.0.0/0	nat-0d7fa27334ad10a4a
10.0.0.0/8	tgw-0c4d0ec54e6f330f1
10.131.44.0/22	local

**Cost**
2022-11
* RDS 8500$
* EC2 2500$
* EKS cluster 140$
* Load Balancer 133$
* VPC 150$
* Cloudwatch/S3 300$
* IOT 300$
total 12000$

# 项目3: Matrix仿真平台

## 驱动接口仿真是怎么回事
* 在真实的交换机上，我们领域的业务代码依赖驱动提供的接口
* 要实现仿真环境的验证，需要对驱动接口进行打桩的工作
* 测试人员需要构造故障, 比如通过改变驱动行为的方式。 如果每次都修改C代码再重新编译, 这种测试体验会很差

## 为什么用Redis不用MySQL
* Redis是基于内存的，性能很高。 我这个项目中的数据规模在2w个key左右，QPS最高在1w左右，因此我们直接使用Redis做数据库
* Redis的key-value模型很适合模拟驱动接口行为，灵活方便, 因为我们的驱动接口就是get/set; MySQL是关系型的，还要先建表, 不灵活
* Redis数据类型丰富（string, list, set, zset, hash)，mamcached只支持string, mongodb是文档型数据库，更适合文档存储
* Redis支持发布订阅和数据库通知 。我在项目中使用这两个特性，解决了仿真环境中单板插拔流程的验证问题

## 读写Redis库怎么做的
基于Hiredis这个C语言实现的开源库, 开了一组读写Redis key的API. 选用HiRedis是因为我们被测代码也是so库, 集成很方便 
举例： 把1号单板设置成不在位，通过修改Redis key: "/board#1/present_status"的值实现，无需修改驱动代码

## Redis挂了怎么办, 可靠性怎么考虑
每个单板1个容器，作为redis client; 1个管理容器，安装redis数据库
如果Redis挂了，管理容器会检测到异常，并依次重启管理容器和各个单板容器。
由于这是仿真环境大部分驱动数据是易失性的，无需持久化或集群支持; 对于需要持久化数据, 通过写文件和Docker Volume方式存储

## 利用Redis的键空间通知机制, 开发了一组发布订阅的API
设备管理的业务依赖驱动接口注册一个回调, 从而感知单板插板事件
我们需要仿真驱动接口，保证发生插拔事件可以触发这个回调, 实现单板插板特性在仿真环境的而测试
如果是轮询的方式，定时检查Redis中单板在位状态, 这种方式浪费CPU.

**如何解决的？成果：**
利用Redis的键空间通知机制解决这个问题:
在驱动函数中, 向Redis发起频道订阅, 频道的名称可以是"Board1/present_status", 订阅单板的在位状态
然后模拟单板插入动作，更新Redis键, 触发Redis键空间通知机制。 Redis将消息发布到所有在线的消费者(也就是我们的上层业务)。 
上层业务收到订阅消息后，触发一次业务回调，从而感知到了单板插板。

## 3. 把Redis短连接优化为长连接怎么回事
早期的仿真平台, 驱动接口是通过Redis短连接实现的，导致频繁地建立连接和关闭连接，性能低下。 
设备管理有一段子卡初始化代码，需要查询很多属性, 用短连接耗时很长

**怎么解决的**
* 通过netstat发现TIME_WAIT过多，都是6379端口，判断使用Redis短连接
* 把短连接改造为长连接。 每个进程分一个Redis长连接，各线程通过pthread互斥锁获取长连接
* 将驱动接口平均一次读写时间缩短为原先的1/15，从平均2ms到0.1ms一次，获得显著性能提升。

## 工具是你一个做的？还是合作的？怎么合作的？
这个工具是合作完成的
早期设计阶段, 有1个架构师, 1个SE, 我参与项目设计和方案讨论, 梳理业务做原型验证，和架构,SE讨论方案。
连我在内, 有两个开发，另一个开发来自杭州。 我负责驱动接口仿真和Redis，另一个同事负责Docker镜像, 仿真设备包构建

后期的性能优化也是合作的，连我在内有3名开发，在同一个部门的业务组。
我做方案设计, 梳理流程拓扑, 把开发任务分解到2名开发同事去做。

## 你觉得还有什么优化
* 优化内存。比如对字符串键做压缩存储（因为Redis字符串键的三种编码: long, embstr,raw）
* 优化请求速度，有些驱动函数会一次性读写多个key，用mset, mget命令代替get, set, 减少客户端和Redis的通信次数

## 举一个问题定位案例
当时遇到一个Redis key被意外删除的问题 

我们通过常规的查看日志, tcpdump抓包方法, 没有找到原因.
我当时看过REDIS的单机数据库实现, 想到既然Redis是基于内存的数据库，字符串键肯定在进程中的某个内存地址处，key丢失时这个地址的内容肯定被改写
所以我只需要GDB打个断点，看下调用栈不就搞定了
后来发现是REDIS配置项maxmemory过低, 只给了1M, 导致Redis认为空间不够就随机淘汰了一些key, 修改了这个配置项后问题得到了解决