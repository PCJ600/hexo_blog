---
layout: next
title: 项目问题梳理
date: 2020-01-02 17:56:30
categories: interview
tags: interview
---

<!-- toc -->
<!-- more -->

# 项目: Trend Micro - Service Gateway

## 简单介绍项目
Service Gateway是一款部署在企业网络中的服务网关, 采用混合云架构, 降低客户的带宽消耗和云服务费用. 
该服务网关由一个云端管理平台和一组基于Rocky Linux的虚拟网关设备组成, 支持VMware, Hyper-V, AWS多平台部署  
网关设备通过Microk8s集群方式运行在客户本地环境, 提供多种XDR(扩展检测与响应)服务, 服务于5万家企业客户, 覆盖1亿台终端设备  

## 1.为On-premises网关设备设计了基于Linux双系统分区的升级方案, 涉及升级包构建, 网络配置同步, GRUB启动项设置等环节. 此方案解决了系统故障后无法回滚的问题, 并删除了1GB的增量升级代码, 降低了90%维护成本

**主要难点是什么, 如何解决的**
在早期，客户的On-premises网关设备使用增量升级方案进行固件更新, 这个方案存在一些严重的问题, 比如:
升级到新版本后出现故障，无法回滚到之前的版本, 客户只能重装系统，用户体验差。
为了保证所有客户都能升级到最新版本, 升级包必须保存从初始版本到最新版本的所有增量文件;
我接手项目时升级包已经达到1G, 随着版本迭代，升级包变得越来越大，难以维护。

为了解决这些问题, 我设计了一个基于双系统分区的全量升级方案, 难点包括升级包构建, 升级前后配置怎么同步, GRUB启动项怎么配置.

**构建升级包**
难点1: 分区如何设计? 
为了支持双系统, 需要对磁盘重新分区, 怎么保证两个分区独立运行互不干扰, 升级之后用户数据不会丢失.
我们的做法是, 用LVM逻辑卷将磁盘分出四个逻辑分区, root和back作为两个系统分区(50G)，data分区存储公共数据，image分区(20G)存储镜像
用逻辑卷是为了客户后期可以扩容.

难点2: 构建升级包过程中, 如何保证文件系统完整性和一致性, 怎么样用最小代价 
导出虚拟机OVA文件, 挂载VMDK, 再对整个文件系统打包, 计算sha256sum (直接在运行机器上打包会出现不一致的问题)
注意: 打包时保留numericID(目标系统新增用户时, 用户UID与当前系统可能不一致), 和文件扩展属性xattr(保留snap运行状态)

**网络配置同步**.
难点: 升级前后需要同步很多配置, 确保无缝升级, 其中网络配置的同步比较复杂, 通过简单复制文件方式难以实现同步
比如说某些客户的设备配置了双网卡, 这些设备从CentOS升级到Rocky, 概率出现网络不通问题, 升级后eth0和eth1两个网卡顺序错误, 导致网络不通

原因: 现代Linux系统使用基于硬件信息（如PCI总线地址）的可预测网卡命名规则，保证OS升级后网卡顺序的正确性。 
但是我们的网关设备采用的是传统的eth网卡命名规则，好处是客户配置网络时无需指定网卡名称, 但是这种规则并不能保证网卡顺序的正确性。
解决方法: 升级前用ethtool记录每个网卡的总线信息businfo, 如果升级后网卡的总线不正确, 用modprobe按正确顺序加载网卡驱动; (如果两个网卡驱动相同, 还需要修改udev规则)

**GRUB启动项设置**
难点: 当两个分区的GRUB版本不一致的时候, 如何正确生成两个启动项
我们选择安装较新版本的GRUB, 因为新版本的引导程序可以兼容旧的CentOS 7, 步骤：
* 先备份当前/boot分区
* 使用目标系统的/boot引导两个系统的内核。
* 通过chroot进入目标系统, 配置并安装GRUB
* 如果在这个过程中GRUB配置失败，通过之前备份的/boot分区内容自动恢复到升级前状态

**遇到了哪些问题**
问题1: 升级后用户无法登录, Microk8s启动失败 (打包时没有添加numeric-owner参数, 没有保留snap的xattr属性)
问题2: 升级后网络不通, 双网卡顺序颠倒
问题3: GRUB配置了10秒超时, 但是重启后启动菜单没有显示 (找了一份源码编译, 打印日志, 发现问题, 是menu_auto_hide环境变量问题

## 2. 通过定制initramfs进入紧急模式, 预先将升级包备份到内存后再重建磁盘分区, 实现了从单系统分区到双系统的无缝迁移, 大幅提升了用户体验

**主要难点是什么, 如何解决的**
我们设计了一个双分区的升级方案, 但是早期客户设备只有一个系统分区。 我们要解决一个问题: 如何让客户的网关设备从单分区无缝迁移到双分区. 它的难点在于:
如果直接在原系统上重新划分分区，会导致数据丢失, 因为根分区在系统启动后是挂载状态，无法直接卸载或修改

为了解决这个问题, 我设计了一个基于initramfs的原地迁移方案
通过定制initramfs进入紧急模式，先挂载磁盘, 把升级包从磁盘复制到临时内存
再重新建立双分区, 解压临时内存中的升级包到根分区, 同步客户配置
最后重新配置和安装GRUB, 执行reboot完成OS迁移

实施这个方案的时候, 遇到一个难点:
initramfs使用的是内存空间, 而客户机器内存是有限的, 最低内存只有8G, 必须保证升级包及其解压后文件远小于8G, 避免因为内存不足导致迁移失败. 我们采用了一些做法，比如:
* 使用CentOS 7 miminal ISO做升级包, 这个ISO只有900M
* 导出虚拟机时, 只分配必要磁盘空间, 等客户机器迁移成功后再自动分配剩余空间
* 清理临时文件, 日志文件, 禁用交换分区
* 使用压缩比率高的算法, 我们选的xz
最终我们的升级包大小是1.7G, 实测运行内存在4G左右, 没有出现客户因内存不足导致的迁移失败

**为什么用这个方案, 有没有别的方案**
平行迁移, 部署一套全新的系统，然后将数据和配置从旧系统迁移到新系统。 我们没有采用, 因为需要客户手动安装配置设备, 做不到无缝迁移

**遇到了哪些问题**
仍然有少量客户(几十个）迁移失败, 比如:
* 磁盘有坏道, 这种只能建议重装
* 客户网络问题, 升级后因为网络原因没有注册, 让客户手动注册解决问题
两万客户, 迁移失败的几十个, 迁移成功率99.5%以上 

## 3. 开发了一个运行在网关设备上的控制模块, 利用AWS IoT消息队列实现在线升级等功能, 并通过心跳检测方法解决微服务同步的问题; 引入预下载机制提高升级的稳定性

**主要难点是什么, 如何解决的**
为了实现虚拟设备的在线升级, 服务安装等功能, 我们开发了一个运行于网关设备上的控制模块, 控制模块包含一个Python的后台进程和cronjob脚本.
后台进程订阅AWS IoT消息队列, 接收云端的升级消息, 然后根据消息类型把任务发送到到相应的Python队列中, 由不同线程处理任务
对于固件升级, 服务安装这类耗时长任务, 我们生成一个taskfile到磁盘, 交给cronjob处理

最大的难点是, 如何在客户设备完成固件升级后, 自动恢复客户之前安装的微服务, 从而实现无缝升级.
为了解决这个问题，我们引入了心跳检测机制: 让云端每5分钟通过消息队列给网关设备发送一次心跳消息, 设备完成升级后，会在心跳响应中带上版本号, 
云端收到响应后通过检查版本号确认升级完成，再下发安装微服务的消息, 确保升级过程对用户来说是无感知的

**为什么引入消息队列, 为什么选择AWS IoT**
实现云端和网关设备之间的解耦通信, 云端只需要把任务发送到消息队列, 无需和每个虚拟设备直接交互, 降低耦合度，提高可维护性
实现异步处理, 云端无需等待任务执行完成, 从而提高系统响应速度

稳定性和可靠性, AWS IoT由Amazon提供的成熟方案, 我们后端也是基于AWS EKS部署的, 所以使用AWS IoT解决方案
易于部署和维护, AWS IoT是完全托管服务, 不需要部署实例
成本低, 费用就是消息传输费(每百万条价格1美元), 所有site每月300美金
不选择RabbitMQ和Kafka: 费用较高, 按照实例和存储收费, 增加复杂性, 有额外成本

**预下载机制**
升级时候发现一个问题, 有很多用户会在同一个时间设置定时升级, 集中下载造成客户瞬时网络流量大, 增加升级失败概率
为了解决这个问题, 我们设计了预下载机制, 把下载升级包和执行升级两个过程分开, 对不同的客户设置随机的预下载时间，
定为发布前12小时的某一个时间点, 分散了客户下载行为，提高稳定性

**遇到了哪些问题**
客户NTP不稳定, 系统时间不正确导致AWS IoT连接失败
解决方法: 控制模块通过一个线程，请求后端获取IoT连接状态, 如果是断连状态, 且系统时间差超过半小时, 立即设置系统时间

## 4.优化了微服务的集成方案, 将微服务从虚拟设备固件中解耦, 并使用Microk8s Ingress提供统一的访问入口, 将虚拟设备镜像从4.5G压缩至2.7G, 客户安装时间缩短了40%

**主要难点是什么, 如何解决的**
在早期版本中, 微服务是通过RPM包方式集成在网关设备固件中, 导致网关设备镜像非常大, 有4.5G左右, 客户安装很慢, 体验很差。 
为了解决这个问题, 我们优化了微服务的集成方案, 将微服务从网关设备固件中解耦
首先设计了一个统一的部署标准, 要求每个微服务团队提供一个标准化的tar.gz的部署包, 包括Docker镜像和YAML资源两部分, 发布到AWS S3上
每个微服务分配了一个serviceCode，网关设备根据这个serviceCode创建同名的Namespace, 实现各微服务间的资源隔离
为了从集群外部访问服务, 我们使用Microk8s Ingress插件作为统一入口, 所有HTTP请求通过宿主机的80和443端口进入, 并基于URL路径进行路由, 避免端口冲突问题

**为什么用Ingress**
Microk8s原生支持Ingress插件, 部署方便. Ingress可以实现基于路径的路由, 为外部访问集群内服务提供统一入口,
使用hostnetwork模式, 所有请求通过宿主机的80和443端口进入, 避免多个微服务端口冲突

**遇到了哪些问题**
小概率出现升级失败, namespace不退出, workaround检测异常强制删除

## 5. 怎么使用诊断命令定位网络问题
网络问题(SG断连):
ping测试, nslookup查DNS, 查看网卡配置, 路由表, iptables规则, curl -v查看输出, 检查代理

## 项目中还有什么优化的
* 优化微服务的升级流程, 使用Helm Chart部署微服务, Helm Upgrade实现滚动更新
* 精简微服务基础镜像, 使用轻量级基础镜像, 提升微服务安装速度

## 项目中团队成员组成
10人团队:
1名项目经理, 负责整体项目规划, 识别需求, 管理进度
1名资深工程师, 为整个团队提供技术指导, 参与关键设计方案的讨论
3名开发, 我负责网关设备特性开发, 1位同事负责前端，1个同事负责后端
2名QA和OPS, 负责测试, 版本发布
其余几名同事负责其他微服务的开发

## 你做了哪些部分, 哪些是合作的, 如何合作
和团队资深开发讨论设计方案
和后端同事讨论虚拟设备和云端的通信方案, 比如消息格式, REST API
和QA合作, 每次实现一个新功能或修复BUG时, 向QA详细说明修改内容和测试方法, 从而提高测试效率
和TS合作,处理客户案例时, 积极和TS沟通客户的问题, 提供解决方案
Stunnel加密方案, Cloud proxy部分是别的团队做的

# 项目2: Forward Proxy Service

## 简单介绍项目
Forward Proxy Service是一个部署在服务网关上的正向代理服务, 为客户终端上云提供统一出口. 它解决了客户需逐一为每个终端配置防火墙策略的痛点, 同时实现了一个Air Gap（终端网络隔离, 通过代理上云）的解决方案. Forward Proxy单个服务实例支持3万个并发连接  

## 基于Squid搭建了支持Basic认证和白名单配置的正向代理服务, 通过ACL规则拦截非法请求, 增强了系统的安全性

**如何实现Basic认证**
Basic认证就是用户名密码认证, 通过HTTP头 Proxy-Authorization携带用户名密码 
每个客户有一个APIKEY(UUID), 对user+apikey做SHA1算法加密得到密码, 

**为什么需要白名单, 如何实现的**
比如说客户某个终端感染了恶意软件，试图访问外部的恶意网站，白名单机制会直接阻止这些请求，防止客户防火墙没有配置策略导致可疑请求泄露.
利用Squid的ACL(Access Control List)功能实现了基于URL的访问控制, 预设了一个FQDN的白名单, 支持域名和IP, 域名支持开头通配符，比如`*.trendmicro.com`
客户可以通过UI添加白名单, 白名单规则存储在ConfigMap, ConfigMap挂载到Pod文件系统上
我们在Pod中做了一个agent进程, 检测到白名单配置更新时, 修改Squid配置文件, 重新加载Squid

**为什么选择Squid, 有没有其他方案**
选择Squid, 是综合考虑开发难度, 成熟度, 社区支持, 功能需求
Squid是一个老牌的正向代理, 在企业场景中经过了长期验证, 成熟稳定, 社区支持好
Squid支持正向代理、基于 URL 的访问控制、Basic 认证, 配置简单, 无需插件
Squid能够支持数万的并发连接, 符合我们需要
没有选Nginx, 因为需要额外模块实现正向代理, ACL, 配置较复杂

**单个服务实例支持高达3万个并发连接, 如何保证这种并发能力的**
事件驱动模型: 通过异步非阻塞方式处理多个连接, 基于epoll和非阻塞IO, 一个进程处理上万连接
多进程架构： 通过多进程架构进一步并发能力。 主进程监听客户端连接, 将连接分发给多个工作进程, 充分利用多CPU计算能力
内存优化: 使用内存池减少频繁内存分配释放, 对HTTP响应进行缓存

**惊群现象是什么，怎么保证每个新连接只会一个工作进程处理**
多个进程都在等待同一个事件, 一旦事件发生, 所有进程都会唤醒, 但实际上只有一个进程能成功处理该事件, 其他进程则会阻塞或重新等待, 导致不必要的性能损失
Squid启用SO_REUSEPORT, 由内核自动将新连接分配给监听同一个端口的不同进程

Nginx中通过accept_mutex防止惊群现象, 每个工作进程都有机会获得锁, 一旦某个工作进程持有锁，其他进程不会尝试接收新连接, 知道进程释放锁
Nginx启动时分配一块共享内存区, 工作进程都可以访问这块内存，工作进程通过原子操作CAS获得锁

### 分析并迅速解决客户网络不通的问题, 涉及HTTP隧道和TLS握手原理

**为啥代理不用HTTPS的呢**
因为HTTP就可以跑TLS流量, 没必要用HTTPS代理
用HTTPS还有生成HTTPS证书, 维护证书, 引入不必要复杂性

**说一下HTTP隧道的原理**
HTTP隧道解决一个问题, HTTP proxy怎么代理HTTPS的流量, 它的流程是:
* 客户端和代理服务器三次握手, 建立TCP连接
* 客户端发送HTTP CONNECT请求给代理, 告诉代理自己需要连接的目标服务器
* 代理收到请求后, 和目标服务器建立TCP隧道
* 代理返回200 Connection Established给客户端, 告诉客户端整个隧道已经建立
* 隧道建立后, 代理服务器只负责在客户端和服务间之间转发数据，不解析或修改数据。这保证了 HTTPS 数据的安全性，即使代理也无法解密 TLS 加密的内容（因为代理不知道密钥, 没有服务器的私钥)

**说一下TLS握手的原理**
HTTPS是基于HTTP的安全协议, 通过在HTTP和TCP层加入了TLS协议实现数据加密, TLS握手步骤:
* 客户端发送ClientHello，包含支持的TLS版本, 加密套件, 客户端随机数
* 服务器返回serverHello，选择双方都支持的TLS版本和加密套件，并附带自己的随机数和证书
* 客户端校验服务器证书, 如果校验通过
* 就根据选定的加密算法(RSA或DH算法)，客户端和服务端协商一个对称的会话密钥
* 最后双方使用协商出的对称密钥进行加密通信

**详细描述下客户网络不通的问题, 如何利用这些原理解决网络问题的**
难点: 
网络不通问题时, 可能有多个故障点, 包括终端设备, 网关设备, 客户防火墙, 目标服务器等. 
每个客户网络环境都是不确定的, 这增加了问题定位难度

我们遇到典型案例包括：
案例1: 端点访问云端失败, Squid代理日志显示 200 OK, 但抓包发现TLS握手失败, 没有发Server Hello
客户终端通过Forward Proxy代理访问云端服务时网络不通. 从Squid日志中看到请求状态码为200 OK，但响应数据包大小仅为40多个字节，远小于正常预期值
让终端同事配合复现问题, 在网关设备上用tcpdump抓包, 发现TLS握手过程中, client hello发送了，但是没有收到server hello, 这说明TLS握手存在问题
用wireshark查包内容，发现客户端和目标服务器使用的TLS加密套件不匹配, 导致TLS握手失败

案例2: 有的客户防火墙开启了HTTPS监控, 做了中间人, 导致证书校验不通过
当防火墙作为中间人拦截HTTPS流量时，它会用自己的证书替换原始服务器的证书, 我们网关设备不会信任客户防火墙证书, 所以无法建立连接
我们在CLI上开发了一个诊断命令行, 通过curl打印请求详细信息，如果证书是一个IP或者防火墙厂商，说明客户防火墙做了中间人, 让客户关闭这个功能

案例3: 访问特定FQDN时出现超时或返回5XX错误(502 Bad Gateway, 503 Service Unavaliable)
原因是客户防火墙配置有问题, 没有放行这个FQDN, 如果丢弃报文就是超时，拒绝就会返回5XX错误

访问了白名单之外的FQDN, 返回 403
代理认证不通过, 返回407
访问的FQDN被反火枪阻挡, 5XX错误 (超时或拒绝)

### 设计了一个基于Stunnel的加密通信方案, 显著减少了因客户防火墙配置异常导致的网络问题, 极大提升了用户体验

**背景**
在本地终端访问是云端资源时，涉及到大量的FQDN（例如 *.trendmicro.com）。
然而，一些客户的防火墙比较老旧，不支持基于FQDN的通配符匹配。
这些客户必须手动逐个配置每个FQDN，这不仅繁琐，而且容易遗漏或出错，导致网络被防火墙阻挡，严重影响用户体验。

**方案**
针对这个问题，我们设计了一种基于Stunnel的加密通信方案. Stunnel是一个开源软件，提供TLS加密服务; 我们使用Stunnel绕过客户防火墙的限制。
部署架构: 在客户防火墙内部署Squid + Stunnel Client, 在防火墙外部部署Stunnel Server + Squid

由于流量都通过Stunnel加密通道传输，客户不再需要在防火墙山上给逐一指定目标服务器的FQDN，只需允许通往Stunnel服务器443端口的流量即可
避免了防火墙通过明文HTTP CONNECT报文或TLS握手过程中的Client Hello报文中的SNI字段识别目标服务器的问题。

**为什么不能只用Squid？**
如果仅在防火墙外部部署Squid, 防火墙可以通过分析HTTP CONNECT报文识别目标服务器，从而对特定域名进行拦截
TLS握手过程中Client Hello报文中的SNI字段也会暴露目标服务器信息，导致防火墙能够识别并拦截特定域名

**有哪些改进的地方**
尽管Stunnel提供了强大的加密功能，但在高并发场景下可能会出现性能瓶颈。
优化措施是水平扩展：通过增加更多的Stunnel实例（SG）来分散负载，提高系统的整体处理能力。

**Squid日志和监控**
为了确保系统的稳定运行，我们实施了以下日志记录和监控措施：
* 健康检查：使用Kubernetes的livenessProbe机制，定时检测Pod的健康状态。定义一个HTTP GET请求，每隔一段时间检查一次Squid的运行状态。
* 日志管理：通过Kubernetes的PersistentVolume机制, 将Squid的日志挂载到宿主机，便于后续问题定位

**如何测试的**
8vCPU/12G/500G/1000Mbps
1000台agent并发升级, 流量157M*1000, 30分钟下载完毕
CPU 8%, Mem 58%, Disk 1%，峰值内存7G，瓶颈在带宽
157*8/(30*60) = 700Mbps, 相当于1Gpbs网卡

# 项目3: Matrix仿真平台
Matrix是一个基于Docker的单板仿真平台, 运用XML建模技术和Redis驱动仿真, 支持全产品形态四十多块单板的仿真. 该平台使得100多名开发和测试人员无需依赖物理设备即可进行软件测试.  
Matrix平台易于部署，可以轻松安装在每个开发人员的Linux机器上，显著提升了日常开发和测试效率, 并节省了物料成本。  

**背景**
在设备管理领域，传统的测试方法通常需要几台设备共享给上百个开发人员使用，每次测试排队时间很长, 测试一次耗时半小时以上，极大地限制了开发效率。
为了解决这一痛点，我们开发了Matrix仿真平台, 让每个开发可以在自己工作机器上快速测试代码, 节省排队和测试时间, 提高开发失效率
作为设备管理部门，我们的业务代码依赖于底层的驱动接口，因此在仿真环境中必须设计一种方法模拟这些驱动接口。 为了满足测试人员灵活构造各种故障的需求，驱动接口不能是硬编码的
整体架构

**Matrix平台的整体架构是什么样的？**
仿真平台部署在开发人员的Linux机器上，包含一个启动脚本, 一组模型文件, 一个仿真设备包
用户执行启动脚本，拉起一个管理容器, 这个管理容器中安装了Redis数据库，用于驱动接口仿真
管理容器读取模型文件, 获取需要拉起的仿真单板信息, 把单板属性写到Redis数据库, 再把相应的单板容器拉起来, 把仿真设备包解压到单板容器 

**XML建模技术具体如何应用的**
XML文件定义每个单板的属性, 比如单板类型, 在位状态,
管理容器启动后, 读取XML文件内容把单板数据以字符串形式写到Redis, 比如1号单板类型对应的key就是"/Chassis$#1/board#1/board_type"

**为什么选择Docker**
项目的主要目标是为开发和测试人员提供一个轻量级的仿真环境，而不是构建高可用、多集群的生产环境。
Docker使用简单，对开发和测试人员来说学习成本低

**为什么用Redis不用MySQL**
Redis是基于内存的数据库，性能非常高。本项目的数据规模大约为2万个key，QPS峰值可达1万左右，Redis可以满足需求
Redis的key-value模型非常适合模拟驱动接口的行为（如get/set操作），而MySQL作为关系型数据库需要先建表，灵活性较差。我们的驱动接口主要依赖于简单的键值操作，因此Redis更加灵活方便
Redis支持多种数据类型（如string、list、set、zset、hash），相比Memcached仅支持string类型，Redis功能更强大；而MongoDB是文档型数据库，更适合文档存储，不适合我们当前的应用场景
Redis支持发布订阅和键空间通知功能，我们在项目中利用Redis特性实现了单板插拔流程的仿真验证，无需引入其他中间件, 使系统简单易于维护

**Redis驱动仿真API的具体功能, 怎么实现动态故障注入***
在设备管理领域，测试人员需要一种简单且可靠的方法来动态修改驱动接口的行为，以便模拟各种故障场景进行测试
传统的做法是利用华为内部的一个C语言库进行函数动态插桩，但在我们设备管理领域，这种方法会出现替换失败的情况，不可靠
为了解决这些问题，我们设计了一种基于Redis的方案，通过Redis动态改变驱动接口行为，实现故障注入

具体来说，我们使用Hiredis（一个C语言开源库）开发了一组读写Redis key的API，用于模拟驱动接口的行为。
由于被测组件也是so库，集成Hiredis非常方便。例如，要将1号单板设置为“不在位”状态，只需修改Redis key "/board#1/present_status" 的值为 false 即可，无需修改驱动代码或重新编译。
相比旧的插桩方案，新方法不仅更加稳定可靠，还极大地简化了故障注入的操作流程，使得测试人员可以快速灵活地构造各种故障场景，显著提高了测试效率和可靠性。

## 通过Redis键空间通知机制, 模拟了单板插拔的仿真, 并通过自定义网桥模拟框式形态的仿真, 扩展了仿真平台的测试范围

**Redis键空间通知机制是如何帮助模拟单板插拔仿真的？**
为了在仿真环境测试单板插拔场景, 需要模拟相关驱动函数行为. 
在真实的设备管理业务中，当发生单板插拔事件时，驱动会通过回调通知设备管理进程。

在仿真环境中，我们需要模拟驱动行为, 如果直接用轮询方式检查Redis中的单板在位状态会导致不必要的CPU开销。 为了解决这一问题，我们利用了Redis的键空间通知机制，具体流程如下：
设备管理进程订阅1号单板是否在位的频道"Board1/present_status', 每当该键发生变化时，Redis都会发送通知。
模拟单板插入时，把表示单板在位的Redis key值置为on, 触发Redis键空间通知
Redis将通知消息发布给所有在线的消费者（即设备管理进程）。设备管理进程收到订阅消息后，触发业务回调，从而感知到单板插拔事件的发生。

利用Redis的键空间通知机制, 实现了单板插板场景仿真, 避免轮询方式带来CPU浪费， 同时没有引入额外中间件, 保持系统的简洁

**接口设计**
```
typedef void (*hiredisSubCallback) (hiredisSubContext *cxt);
int HiredisSubscribe(const char *key, hiredisSubCallback cb, hiredisSubContext *cxt);
```
* 订阅频道 subscribe  __keyspace@0__:/board#1/present_status"
* 创建线程，循环读取redis回复, 收到回复后调用回调


**自定义网桥是如何实现框式形态仿真的？**
在框式设备仿真中，单板间的通信需要使用特定的172.16网段, Docker默认的docker0网桥使用的是172.17
为了避免修改客户Docker配置或重启服务，我们创建了一个自定义网桥，指定其IP段为172.16，将所有单板容器连接到这个网桥上。 通过这种方式，容器间可以直接通过172.16 IP段通信，模拟了真实设备的网络环境。

Docker间通信原理是，每个容器通过一对虚拟以太网接口（veth对）与宿主机上的自定义网桥通信
veth pair由两个虚拟网络接口组成，一端位于宿主机的网络命名空间（network namespace），另一端位于容器的网络命名空间。
当容器启动时，Docker会自动创建veth pair，并将其一端连接到容器内部的eth0接口，另一端连接到宿主机上的自定义网桥。
这样，容器内的网络流量可以通过自定义网桥在不同容器之间传输，实现高效的局域网通信。

## 将Redis驱动接口从短连接优化为长连接, 接口平均读写时间从2ms缩短为0.1ms, 显著提升了测试效率

**将Redis驱动接口从短连接优化为长连接**
在项目初期，驱动接口通过Redis短连接实现，导致频繁建立和关闭连接。特别是在设备管理子卡初始化代码中，需要查询大量属性时，影响测试效率
为了解决这个问题，我们将短连接优化为长连接。 通过互斥锁, 让一个进程的所有线程共用一个长连接。 最终将驱动接口的平均一次读写时间从2ms缩短到0.1ms，显著提升了性能。

**如何测量和验证平均一次读写时间从2ms缩短至0.1ms的效果？**
在测试环境, 直接修改C代码, 用time_t计算时间差,


## 其他问题
**举一个问题定位案例**
当时遇到一个Redis key被意外删除, 导致驱动接口行为不符合预期的问题 

我们通过常规的查看日志, tcpdump抓包方法, 没有找到原因.
我当时看过REDIS的单机数据库实现, 想到既然Redis是基于内存的数据库，字符串键肯定在进程中的某个内存地址处，key丢失时这个地址的内容肯定被改写
所以我只需要GDB打个断点，看下调用栈不就搞定了
后来发现是REDIS配置项maxmemory过低, 只给了1M, 导致Redis认为空间不够就随机淘汰了一些key, 修改了这个配置项后问题得到了解决

**你负责哪些部分, 怎么和其他团队成员合作的**
这个工具是合作完成的
早期设计阶段, 有1个架构师, 1个SE, 我参与项目设计和方案讨论, 梳理业务做原型验证，和架构,SE讨论方案。
连我在内, 有两个开发，另一个开发来自杭州。 我负责驱动接口仿真和Redis，另一个同事负责Docker镜像, 仿真设备包构建

后期的性能优化也是合作的，连我在内有3名开发，在同一个部门的业务组。 我做方案设计, 梳理流程拓扑, 把开发任务分解到2名开发同事去做。

**你觉得还有什么优化**
* 优化内存。比如对字符串键做压缩存储（因为Redis字符串键的三种编码: long, embstr,raw）
* 优化请求速度，有些驱动函数会一次性读写多个key，用mset, mget命令代替get, set, 减少客户端和Redis的通信次数

**Redis挂了怎么办, 可靠性怎么考虑**
每个单板1个容器，作为redis client; 1个管理容器，安装redis数据库
如果Redis挂了，管理容器会检测到异常，并依次重启管理容器和各个单板容器。
由于这是仿真环境大部分驱动数据是易失性的，无需持久化或集群支持; 对于需要持久化数据, 通过写文件和Docker Volume方式存储



# 技能描述

## Docker网络模式及容器间通信的原理
Bridge模式是默认的网络模式，为单个主机上的容器提供网络通信服务。当容器启动时，Docker会自动创建一个虚拟网桥docker0，
并通过veth对将容器连接到这个网桥上。每个容器都会获得一个独立的IP地址，并且可以通过网桥进行相互通信
Overlay网络则是用于多主机环境下容器间的通信。它通过在网络层建立一个虚拟网络
比如说使用VXLAN技术封装数据包以跨越多个主机传输

## Nginx Ingress
Nginx Ingress解决的问题是从集群外部如何访问集群内的某个服务
它的原理是, 在Microk8s里运行了一个Nginx Ingress Controller, 监听k8s中的Ingress资源，当检测到Ingress资源更新时, 动态更新Nginx配置文件
外部流量先到达Nginx, 再基于域名和URL将请求转发到Service, 最终把请求转发到具体的Pod

## CoreDNS原理
为Kubernetes集群内部的DNS解析提供服务，使得服务之间能够通过域名互相通信
Kubernetes集群中, CoreDNS是运行在kube-system这个namespace下的Pod
（同一个namespace的服务，直接用名称访问, 不同Namespace下的需要通过svc.namespaceX访问）
ClusterFirst, 优先使用CoreDNS的, 无法解析回退到宿主机 
ClusterFirstWithHostNet, 使用宿主机网络Pod, 仍使用CoreDNS
Default 使用宿主机的DNS

## Calico网络模式
k8s中, 主要解决Pod间通信问题, 支持三种模式, BGP, VXLAN, IPIP
在BGP模式下，Calico 使用标准的 BGP 协议来动态地学习和分发路由信息。
每个节点作为一个 BGP 路由器，直接与其他节点交换路由信息，从而允许容器之间的直接通信，无需额外的封装或隧道技术。这种方式减少了网络开销，提高了性能。
底层网络不支持BGP, 需跨越不同网络时, 使用VXLAN和IPIP模式, VXLAN 把原始二层帧封装在UDP报文, 实现不同网络Pod通信; IPIP模式在原始IP包外面再封装一层IP包, 实现跨子网通信

# 其他项目问题

## 说下Linux启动流程
* 系统加电, BIOS开机自检, 自检完成后, 根据预设的启动顺序查找可启动设备（如硬盘、USB）
* BIOS 找到第一个可启动设备后, 控制权交给引导程序 GRUB2
* GRUB 将选定的内核镜像vmlinuz加载到内存, 挂载临时根文件系统 initramfs
* 内核初始化硬件设备, 通过initramfs加载必要模块, 挂载真实的根文件系统
* 执行用户空间第一个进程systemd, 由systemd负责启动其他进程

## GRUB什么原理
GRUB（GRand Unified Bootloader), Linux系统中广泛使用的引导加载程序, 负责加载操作系统内核到内存并启动
划分3阶段, Stage 1, Stage 1.5, Stage 2, 其中Stage 2是GRUB的核心部分，包含用户界面、配置文件解析器以及加载内核和initrd的工具。
当选择了启动菜单中的某个条目后, GRUB会将指定的kernel和initrd加载到内存中, 内核被加载到内存后, 控制权转移给内核, 继续启动流程

## initramfs是什么
initramfs是一个临时根文件系统, 用于内核启动初期提供必要驱动程序, 让内核能够挂载真正根文件系统
因为内核本身不会包含所有的驱动程序, 实际根文件系统可能位于复杂存储设备上(比如LVM), 这些设备需要特定驱动才能访问呢

## 怎么保证消息不丢失
生产者角度
* 要有重试机制; 云端在业务代码中判断超时后重试发送多次
* 另外有补偿机制, 将任务记录到数据库, 通过cronjob检查数据中未收到响应的任务, 进行补偿重试(重试3次, 每次间隔5分钟) 
中间件角度
* 开启持久会话(cleanSession=false), 客户端断开链接后, 消息也不会丢失
* 设置Qos 1(至少一次传递), 消息会被保留直到确认或超时
消费端角度
* 收到消息后并处理完成后, 发送ACK给消息队列, 让消息队列及时清除消息

## 如何避免消息重复消费
幂等设计, 无论消息被处理多少次, 结果都是相同的
过期字典, 每次消费时先检查taskId是否在过期字典中; 如果ID存在说明消息重复; 否则消费消息之后，把taskId添加到过期字典。

## 如果升级版本有问题, 怎么最小化用户影响
设计灰度升级方案, 我们的服务网关部署在全球多个地区(比如日本,欧洲,新加坡)。 
为了防止升级失败对所有客户造成影响, 我们首先选择客户较少的地区发布, 先指定少量客户升级(通过launchDarklyKey API), 再让整个地区的升级
对于有多台虚拟设备的客户, 也采用分批升级。 首次只升级一半设备, 等下一个周期后先判断这些设备是否升级成功, 如果升级成功再升级剩余一半

## JWT注册流程
* 客户在UI上拿到一个由服务平台维护的register_token
* 用户在命令行中输入register_token注册，触发注册请求
* 虚拟设备通过POST请求将CPU,内存,IP信息连同register_token一起发送给后端
* 后端收到请求, 解析请求头的token, 验证token, 获取customerId
* 校验通过后, 后端为虚拟设备生成一个uuid, 用于唯一标识这台设备, 再生成一个applianceToken
* 后端把虚拟设备信息存到MySQL数据库, 把虚拟设备ID, Token, 还有消息队列的FQDN返回给虚拟设备
* 虚拟设备成功收到响应后，保存applianceId, applianceToken, 用于后续通信   
每天定时从服务平台同步Token, Token设置了两个月过期时间, 过期前10天，立刻请求服务平台刷新Token

## JWT工作原理是什么 
JWT用于身份认证, 通过签名确保数据完整性和真实性
由三部分组成, Header, Payload, Signature, 工作原理:
* 用户登录成功后，服务器生成一个JWT返回给客户端
* 后续请求中, 客户端将JWT附加到HTTP请求头中, 发送给服务器
* 服务器接收到JWT后, 验证其签名和有效性
优点: 
* 无状态, 服务器无需存储会话, 适合分布式系统
* 跨平台, JWT基于JSON格式, 易于解析使用, 支持多种编程语言
局限性:
* 无法主动失效

## JWT和Session
JWT无状态的token, 服务端不需保存; Session是有状态的, 需要存储Session识别用户
JWT是无状态的，它更易于扩展, 但是难以撤销, 通常通过HTTP头传输， Session依赖于cookie传输Session ID

## Cookie和Session认证问题
1、用户向服务器发送用户名和密码。
2、服务器验证通过后，在当前对话（session）里面保存相关数据，比如用户角色、登录时间等等。
3、服务器向用户返回一个 session_id，写入用户的 Cookie。
4、用户随后的每一次请求，都会通过 Cookie，将 session_id 传回服务器。
5、服务器收到 session_id，找到前期保存的数据，由此得知用户的身份。

# 简历中的技术名词

## 为什么采用混合云架构?
混合云指的是将私有云环境与公有云服务相结合的一种云计算模型，我们的服务网关采用了混合云架构, 由一个云端的管理平台和一组部署在客户本地的网关设备组成
在客户的私有云环境中, 我们通过Microk8s集群为客户终端提供服务, 减少了客户的公网带宽消耗
但是, 如果我们只有私有云, 网关设备的更新会变得难以实现，所以我们通过公有云上部署一个管理平台，实现网关设备的升级

## Microk8s集群是指什么? 你们为什么不用高可用集群? 
我们的Microk8s集群是由多个独立运行的Microk8s示例组成的集群，每个节点处理各自终端的请求
没有使用高可用集群, 主要是几个方面考虑：
一个是实现的复杂性增加, 实现高可用集群, 需要引入额外的负载均衡器, 涉及到如何管理和部署这些LB, 还需要分配一个VIP, 增加了复杂性.
另一个是资源消耗, 高可用集群一般要求至少三个主节点以及多个工作节点, 消耗更多的资源, 以及更高的维护成本

## 如果是高可用集群, 你准备怎么部署?
高可用集群, 需要部署至少三个主节点, 加上多个工作节点, 前置负载均衡器(LB), 负载均衡器可以使用HAProxy或者LVS
每个LB上配置keepalived管理VIP, 客户终端通过VIP访问服务网关

## XDR是什么意思
Extended Detection and Response 扩展检测和响应，是一种安全解决方案。 
它在EDR（端点检测与响应）的基础上进行了扩展，不仅关注端点的安全，还整合了网络、云端, 邮件等多个层面的数据进行威胁检测和响应, 为企业客户提供全面的安全防护

## LocalActiveUpdate
LAU是一种容器化的服务, 部署在我们的服务网关上, 为趋势科技的本地终端产品（如Apex One、DDI和DSM等）提供最新的威胁情报库更新。
LAU服务定期从云端服务器下载最新的病毒库和扫描引擎等组件, 把这些更新内容存储在本地服务器上，客户终端只需从本地获取更新, 这样大大减轻了网络负担

## WebReputation
WebReputation是一个用于评估网页安全性的服务。 通过给访问的网站打分帮助用户避免安全威胁。
比如说，像Google, MS这样网站会给予高分, 并列入白名单; 恶意站点, 和白名单相似的可疑网站, 会给一个低分
这些数据来源于公司的爬虫服务, 还有从三方购买的一些数据

## FPS中, 逐一为端点配置防火墙策略痛点指的是什么
客户必须在自己的防火墙上手动配置大量的云端服务URL, 使得本地终端产品能够连接到云端服务; 由于
客户的本地终端需要访问很多云端服务的URL, 客户环境中运行了大量的终端设备, 让客户为这些终端逐一指定防火墙策略是非常麻烦的,
而且万一配置出现遗漏, 本地终端发出恶意请求, 导致安全问题
解决方案: 通过FPS, 所有本地终端产品可以通过单一代理服务访问后端服务, 客户只需要在防火墙上配置允许通过FPS的URL规则, 无需为每个本地终端单独设置规则, 显著简化了防火墙策略配置工作量

## Air Gap是啥意思
对于银行这种敏感的客户, 其内部网络与外部网络之间是隔离的, 即所谓的Air Gap环境, 他们的终端设备无法直接连到外网 
为了让这些终端也可以连到云，我们设计了Forward Proxy解决方案, 客户把我们的代理部署在DMZ区域, 让所有终端设备的出站请求都经过我们代理进行转发, 通过代理上云

## 什么是云计算
云计算是一种通过互联网提供计算资源和服务的技术, 允许用户按需访问和使用资源，无需购买维护硬件设备
为企业部署服务提供了灵活性, 可扩展性, 只需要为使用的资源付费, 降低成本

## 公有云和私有云
公有云是由第三方云服务提供商提供的云计算环境, 比如AWS, Azure, 阿里云, 按需付费, 无需购买硬件设备，自建数据中心, 提高了部署的灵活性, 降低了成本
私有云是专门为单个组织构建的云计算环境, 位于组织的数据中心内，也可以托管给第三方服务商，使用VMware, Hyper-V实现资源虚拟化, 私有云适合那些对安全性要求较高的企业, 比如金融医疗行业

## 基础设施即服务(IaaS), 平台即服务(PaaS),  软件即服务(SaaS)
SaaS模式, 通过互联网我提供软件应用, 用户无需安装软件, 而是直接使用服务. 比如Office 365, 用户通过浏览器直接使用, 所有更新支持由微软提供
PaaS模式, 提供一个开发和部署环境, 用户需要关注自己业务逻辑, 开发应用软件, 但是无需关注底层硬件和操作系统
IaaS模式, 提供虚拟化计算资源, 用户需要自行管理OS, 中间件, 应用软件, 比如 Amazon EC2


# 云服务费用
**DeployMent**
AWS云, 每个site一个EKS, site: US, EU, SG, AU, IN, JP, UAE
**Cost monthly**
* RDS 8500$
* EC2 2500$
* EKS cluster 140$
* Load Balancer 133$
* VPC 150$
* Cloudwatch/S3 300$
* IOT 300$